
training_data_paths:
  - "../../foundational_models/datasets/training/newExchangeRateTraining.arrow"
probability:
  - 1.0
context_length: 240
prediction_length: 240
min_past: 60
max_steps: 10000
save_steps: 2000
log_steps: 200
per_device_train_batch_size: 4
learning_rate: 0.001
optim: adamw_torch_fused
num_samples: 20
shuffle_buffer_length: 10000
gradient_accumulation_steps: 2
model_id: google/t5-efficient-large
model_type: seq2seq
random_init: false
tie_embeddings: true
output_dir: google_t5-efficient-large
tf32: true
torch_compile: true
tokenizer_class: "MeanScaleUniformBins"
tokenizer_kwargs:
  low_limit: -15.0
  high_limit: 15.0
n_tokens: 4096
lr_scheduler_type: linear
warmup_ratio: 0.1
dataloader_num_workers: 1
max_missing_prop: 0.9
use_eos_token: true
